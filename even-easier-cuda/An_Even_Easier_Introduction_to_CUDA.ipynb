{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "An Even Easier Introduction to CUDA.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZZJzhou/cuda_tutorial/blob/main/even-easier-cuda/An_Even_Easier_Introduction_to_CUDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WkOA4mcN7Hj"
      },
      "source": [
        "# An Even Easier Introduction to CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuOcUi0fvogW"
      },
      "source": [
        "This notebook accompanies Mark Harris's popular blog post [_An Even Easier Introduction to CUDA_](https://developer.nvidia.com/blog/even-easier-introduction-cuda/).\n",
        "\n",
        "If you enjoy this notebook and want to learn more, the [NVIDIA DLI](https://nvidia.com/dli) offers several in depth CUDA Programming courses.\n",
        "\n",
        "For those of you just starting out, please consider [_Fundamentals of Accelerated Computing with CUDA C/C++_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/about) which provides dedicated GPU resources, a more sophisticated programming environment, use of the [NVIDIA Nsight Systems™](https://developer.nvidia.com/nsight-systems) visual profiler, dozens of interactive exercises, detailed presentations, over 8 hours of material, and the ability to earn a DLI Certificate of Competency.\n",
        "\n",
        "Similarly, for Python programmers, please consider [_Fundamentals of Accelerated Computing with CUDA Python_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-02+V1/about).\n",
        "\n",
        "For more intermediate and advance CUDA programming materials, please check out the _Accelerated Computing_ section of the NVIDIA DLI [self-paced catalog](https://www.nvidia.com/en-us/training/online/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1C6GK_MO5er"
      },
      "source": [
        "<img src=\"https://developer.download.nvidia.com/training/courses/T-AC-01-V1/CUDA_Cube_1K.jpeg\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcmbR8lZPLRv"
      },
      "source": [
        "This post is a super simple introduction to CUDA, the popular parallel computing platform and programming model from NVIDIA. I wrote a previous [“Easy Introduction”](https://developer.nvidia.com/blog/easy-introduction-cuda-c-and-c/) to CUDA in 2013 that has been very popular over the years. But CUDA programming has gotten easier, and GPUs have gotten much faster, so it’s time for an updated (and even easier) introduction.\n",
        "\n",
        "CUDA C++ is just one of the ways you can create massively parallel applications with CUDA. It lets you use the powerful C++ programming language to develop high performance algorithms accelerated by thousands of parallel threads running on GPUs. Many developers have accelerated their computation- and bandwidth-hungry applications this way, including the libraries and frameworks that underpin the ongoing revolution in artificial intelligence known as [Deep Learning](https://developer.nvidia.com/deep-learning).\n",
        "\n",
        "So, you’ve heard about CUDA and you are interested in learning how to use it in your own applications. If you are a C or C++ programmer, this blog post should give you a good start. To follow along, you’ll need a computer with an CUDA-capable GPU (Windows, Mac, or Linux, and any NVIDIA GPU should do), or a cloud instance with GPUs (AWS, Azure, IBM SoftLayer, and other cloud service providers have them). You’ll also need the free [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) installed.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDQ9ycz0Qfyf"
      },
      "source": [
        "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_ai_cube-625x625.jpg\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH9Rfms_QtXF"
      },
      "source": [
        "## Starting Simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5-iUihBQvQt"
      },
      "source": [
        "We’ll start with a simple C++ program that adds the elements of two arrays with a million elements each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc-gBqLDQ7AC",
        "outputId": "89278e51-95b9-4b2b-d6c5-760f6e86fee2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile add.cpp\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// function to add the elements of two arrays\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20; // 1M elements\n",
        "\n",
        "  float *x = new float[N];\n",
        "  float *y = new float[N];\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the CPU\n",
        "  add(N, x, y);\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  delete [] x;\n",
        "  delete [] y;\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing add.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw6DsX4uRHMg"
      },
      "source": [
        "Executing the above cell will save its contents to the file add.cpp.\n",
        "\n",
        "The following cell will compile and run this C++ program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNpH54M_RbAU",
        "outputId": "e71b34eb-69dd-4ae4-a9ff-a7acd6eba4ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "g++ add.cpp -o add"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6V2tGPYRi3l"
      },
      "source": [
        "Then run it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmA4ACe5RuiU",
        "outputId": "f922d062-e9fc-4fb8-c54c-4bdac51d7dbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "./add"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max error: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IAWYlniR153"
      },
      "source": [
        "As expected, it prints that there was no error in the summation and then exits. Now I want to get this computation running (in parallel) on the many cores of a GPU. It’s actually pretty easy to take the first steps.\n",
        "\n",
        "First, I just have to turn our `add` function into a function that the GPU can run, called a *kernel* in CUDA. To do this, all I have to do is add the specifier `__global__` to the function, which tells the CUDA C++ compiler that this is a function that runs on the GPU and can be called from CPU code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heY-lpzjSHfB"
      },
      "source": [
        "```cpp\n",
        "// CUDA Kernel function to add the elements of two arrays on the GPU\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kozMbHdpSKNu"
      },
      "source": [
        "These `__global__` functions are known as *kernels*, and code that runs on the GPU is often called *device code*, while code that runs on the CPU is *host code*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhnBGGU-SWiN"
      },
      "source": [
        "## Memory Allocation in CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvIDRBk2SbqA"
      },
      "source": [
        "To compute on the GPU, I need to allocate memory accessible by the GPU. [Unified Memory](https://developer.nvidia.com/blog/unified-memory-in-cuda-6/) in CUDA makes this easy by providing a single memory space accessible by all GPUs and CPUs in your system. To allocate data in unified memory, call `cudaMallocManaged()`, which returns a pointer that you can access from host (CPU) code or device (GPU) code. To free the data, just pass the pointer to `cudaFree()`.\n",
        "\n",
        "I just need to replace the calls to `new` in the code above with calls to `cudaMallocManaged()`, and replace calls to `delete []` with calls to `cudaFree`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxCut_urS46H"
      },
      "source": [
        "```cpp\n",
        "  // Allocate Unified Memory -- accessible from CPU or GPU\n",
        "  float *x, *y;\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  ...\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oEf2B-1S-1V"
      },
      "source": [
        "Finally, I need to *launch* the `add()` kernel, which invokes it on the GPU. CUDA kernel launches are specified using the triple angle bracket syntax `<<< >>>`. I just have to add it to the call to `add` before the parameter list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqTJlvWLS7iW"
      },
      "source": [
        "```cpp\n",
        "add<<<1, 1>>>(N, x, y);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGf0ZiTOTTHU"
      },
      "source": [
        "Easy! I’ll get into the details of what goes inside the angle brackets soon; for now all you need to know is that this line launches one GPU thread to run `add()`.\n",
        "\n",
        "Just one more thing: I need the CPU to wait until the kernel is done before it accesses the results (because CUDA kernel launches don’t block the calling CPU thread). To do this I just call `cudaDeviceSynchronize()` before doing the final error checking on the CPU.\n",
        "\n",
        "Here’s the complete code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8bYDM7kYT7S",
        "outputId": "b112caae-6ef3-4088-fc42-d8ddc3591eec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile add.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20\n",
        " ;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory – accessible from CPU or GPU\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  add<<<1, 1>>>(N, x, y);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing add.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjLGGp0oYeEc",
        "outputId": "4d60dfda-9fe4-46a4-e74e-1e987e74c232",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "nvcc add.cu -o add_cuda\n",
        "./add_cuda"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max error: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ATssEzEYqGx"
      },
      "source": [
        "This is only a first step, because as written, this kernel is only correct for a single thread, since every thread that runs it will perform the add on the whole array. Moreover, there is a [race condition](https://en.wikipedia.org/wiki/Race_condition) since multiple parallel threads would both read and write the same locations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kKpDoZ-YzJ8"
      },
      "source": [
        "## Profile it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-BC-CWVZglt"
      },
      "source": [
        "I think the simplest way to find out how long the kernel takes to run is to run it with `nvprof`, the command line GPU profiler that comes with the CUDA Toolkit. Just type `nvprof ./add_cuda` on the command line:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtfQLWwYZpfV",
        "outputId": "46de8749-ed74-4dc2-a9c6-040d62f3257a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "nvprof ./add_cuda"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==11396== NVPROF is profiling process 11396, command: ./add_cuda\n",
            "Max error: 0\n",
            "==11396== Profiling application: ./add_cuda\n",
            "==11396== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  110.29ms         1  110.29ms  110.29ms  110.29ms  add(int, float*, float*)\n",
            "      API calls:   64.94%  206.30ms         2  103.15ms  45.730us  206.25ms  cudaMallocManaged\n",
            "                   34.72%  110.30ms         1  110.30ms  110.30ms  110.30ms  cudaDeviceSynchronize\n",
            "                    0.17%  539.53us         2  269.76us  266.47us  273.06us  cudaFree\n",
            "                    0.10%  328.29us         1  328.29us  328.29us  328.29us  cudaLaunchKernel\n",
            "                    0.06%  192.33us       114  1.6870us     142ns  59.544us  cuDeviceGetAttribute\n",
            "                    0.00%  13.160us         1  13.160us  13.160us  13.160us  cuDeviceGetName\n",
            "                    0.00%  5.9610us         1  5.9610us  5.9610us  5.9610us  cuDeviceGetPCIBusId\n",
            "                    0.00%  5.4650us         1  5.4650us  5.4650us  5.4650us  cuDeviceTotalMem\n",
            "                    0.00%  1.6400us         3     546ns     202ns  1.1830us  cuDeviceGetCount\n",
            "                    0.00%  1.1330us         2     566ns     179ns     954ns  cuDeviceGet\n",
            "                    0.00%     559ns         1     559ns     559ns     559ns  cuModuleGetLoadingMode\n",
            "                    0.00%     471ns         1     471ns     471ns     471ns  cuDeviceGetUuid\n",
            "\n",
            "==11396== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      48  170.67KB  4.0000KB  0.9961MB  8.000000MB  805.7230us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  361.1500us  Device To Host\n",
            "      12         -         -         -           -  4.023321ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9Dn4ZV-Z_UJ"
      },
      "source": [
        "The above will show the single call to `add`. Your timing may vary depending on the GPU allocated to you by Colab. To see the current GPU allocated to you run the following cell and look in the `Name` column where you might see, for example `Tesla T4`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrYmwVZfaPqz",
        "outputId": "246fd4d1-77bf-42f7-a09e-4dbc26906ec4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "nvidia-smi"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 26 06:33:20 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MWYteAVadCs"
      },
      "source": [
        "Let's make it faster with parallelism."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaiMC73Falvb"
      },
      "source": [
        "## Picking up the Threads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDFuBr_2apuJ"
      },
      "source": [
        "Now that you’ve run a kernel with one thread that does some computation, how do you make it parallel? The key is in CUDA’s `<<<1, 1>>>` syntax. This is called the execution configuration, and it tells the CUDA runtime how many parallel threads to use for the launch on the GPU. There are two parameters here, but let’s start by changing the second one: the number of threads in a thread block. CUDA GPUs run kernels using blocks of threads that are a multiple of 32 in size, so 256 threads is a reasonable size to choose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Pmyj0KavgB"
      },
      "source": [
        "```cpp\n",
        "add<<<1, 256>>>(N, x, y);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAYpH9Ctay5G"
      },
      "source": [
        "If I run the code with only this change, it will do the computation once per thread, rather than spreading the computation across the parallel threads. To do it properly, I need to modify the kernel. CUDA C++ provides keywords that let kernels get the indices of the running threads. Specifically, `threadIdx.x` contains the index of the current thread within its block, and `blockDim.x` contains the number of threads in the block. I’ll just modify the loop to stride through the array with parallel threads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSiqhFK_a6N3"
      },
      "source": [
        "```cpp\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = threadIdx.x;\n",
        "  int stride = blockDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7mYcBzOa9zR"
      },
      "source": [
        "The `add` function hasn’t changed that much. In fact, setting `index` to 0 and `stride` to 1 makes it semantically identical to the first version.\n",
        "\n",
        "Here we save the file as add_block.cu and compile and run it in `nvprof` again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goCKY9QNbPZ-",
        "outputId": "ffa5c125-89d6-41aa-b8a6-c36ba5878e06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile add_block.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = threadIdx.x;\n",
        "  int stride = blockDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory – accessible from CPU or GPU\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  add<<<1, 256>>>(N, x, y);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing add_block.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9cmfbcVbYgD",
        "outputId": "94b69f98-9c9a-42b5-aa26-bb6e5923ecca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "nvcc add_block.cu -o add_block\n",
        "nvprof ./add_block"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==18105== NVPROF is profiling process 18105, command: ./add_block\n",
            "Max error: 0\n",
            "==18105== Profiling application: ./add_block\n",
            "==18105== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  3.8497ms         1  3.8497ms  3.8497ms  3.8497ms  add(int, float*, float*)\n",
            "      API calls:   76.86%  239.22ms         2  119.61ms  41.612us  239.18ms  cudaMallocManaged\n",
            "                   21.63%  67.311ms         1  67.311ms  67.311ms  67.311ms  cudaLaunchKernel\n",
            "                    1.24%  3.8573ms         1  3.8573ms  3.8573ms  3.8573ms  cudaDeviceSynchronize\n",
            "                    0.19%  587.75us         2  293.87us  233.61us  354.14us  cudaFree\n",
            "                    0.07%  217.15us       114  1.9040us     207ns  93.620us  cuDeviceGetAttribute\n",
            "                    0.00%  12.221us         1  12.221us  12.221us  12.221us  cuDeviceGetName\n",
            "                    0.00%  7.6840us         1  7.6840us  7.6840us  7.6840us  cuDeviceGetPCIBusId\n",
            "                    0.00%  6.0020us         1  6.0020us  6.0020us  6.0020us  cuDeviceTotalMem\n",
            "                    0.00%  2.0720us         3     690ns     317ns  1.3470us  cuDeviceGetCount\n",
            "                    0.00%  1.1140us         2     557ns     319ns     795ns  cuDeviceGet\n",
            "                    0.00%     574ns         1     574ns     574ns     574ns  cuModuleGetLoadingMode\n",
            "                    0.00%     390ns         1     390ns     390ns     390ns  cuDeviceGetUuid\n",
            "\n",
            "==18105== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      48  170.67KB  4.0000KB  0.9961MB  8.000000MB  810.6810us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  362.2040us  Device To Host\n",
            "      12         -         -         -           -  2.239052ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo5KaV3Nba7g"
      },
      "source": [
        "That’s a big speedup (compare the time for the `add` kernel by looking at the `GPU activities` field), but not surprising since I went from 1 thread to 256 threads. Let’s keep going to get even more performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtgQWOyMcPfn"
      },
      "source": [
        "## Out of the Blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAoFGwmbcRbN"
      },
      "source": [
        "CUDA GPUs have many parallel processors grouped into Streaming Multiprocessors, or SMs. Each SM can run multiple concurrent thread blocks. As an example, a Tesla P100 GPU based on the [Pascal GPU Architecture](https://developer.nvidia.com/blog/inside-pascal/) has 56 SMs, each capable of supporting up to 2048 active threads. To take full advantage of all these threads, I should launch the kernel with multiple thread blocks.\n",
        "\n",
        "By now you may have guessed that the first parameter of the execution configuration specifies the number of thread blocks. Together, the blocks of parallel threads make up what is known as the *grid*. Since I have `N` elements to process, and 256 threads per block, I just need to calculate the number of blocks to get at least `N` threads. I simply divide `N` by the block size (being careful to round up in case `N` is not a multiple of `blockSize`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnI2II2ockgC"
      },
      "source": [
        "```cpp\n",
        "int blockSize = 256;\n",
        "int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "add<<<numBlocks, blockSize>>>(N, x, y);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayq2MJZLctY0"
      },
      "source": [
        "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_indexing.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZduP7RWc3Je"
      },
      "source": [
        "I also need to update the kernel code to take into account the entire grid of thread blocks. CUDA provides `gridDim.x`, which contains the number of blocks in the grid, and `blockIdx.x`, which contains the index of the current thread block in the grid. Figure 1 illustrates the the approach to indexing into an array (one-dimensional) in CUDA using `blockDim.x`, `gridDim.x`, and `threadIdx.x`. The idea is that each thread gets its index by computing the offset to the beginning of its block (the block index times the block size: `blockIdx.x * blockDim.x`) and adding the thread’s index within the block (`threadIdx.x`). The code `blockIdx.x * blockDim.x + threadIdx.x` is idiomatic CUDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cI2WLEAeG5y"
      },
      "source": [
        "```cpp\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83hC-rCLdPHC"
      },
      "source": [
        "The updated kernel also sets stride to the total number of threads in the grid (`blockDim.x * gridDim.x`). This type of loop in a CUDA kernel is often called a [*grid-stride*](https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/) loop.\n",
        "\n",
        "Save the file as `add_grid.cu` and compile and run it in `nvprof` again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7w-DHBRdhUC",
        "outputId": "b3ab9c1a-02a2-4961-b72d-47f8c56ccc10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile add_grid.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory – accessible from CPU or GPU\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  int blockSize = 256;\n",
        "  int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "  add<<<numBlocks, blockSize>>>(N, x, y);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing add_grid.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhcrktW9dw34",
        "outputId": "0ac1f984-dffb-4116-b1ba-8d4e9508a880",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "nvcc add_grid.cu -o add_grid\n",
        "nvprof ./add_grid"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==838== NVPROF is profiling process 838, command: ./add_grid\n",
            "Max error: 0\n",
            "==838== Profiling application: ./add_grid\n",
            "==838== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  2.6598ms         1  2.6598ms  2.6598ms  2.6598ms  add(int, float*, float*)\n",
            "      API calls:   63.51%  235.50ms         2  117.75ms  39.684us  235.46ms  cudaMallocManaged\n",
            "                   35.11%  130.18ms         1  130.18ms  130.18ms  130.18ms  cudaLaunchKernel\n",
            "                    1.21%  4.4736ms         1  4.4736ms  4.4736ms  4.4736ms  cudaDeviceSynchronize\n",
            "                    0.13%  480.65us         2  240.32us  214.07us  266.58us  cudaFree\n",
            "                    0.04%  160.94us       114  1.4110us     171ns  55.857us  cuDeviceGetAttribute\n",
            "                    0.00%  12.255us         1  12.255us  12.255us  12.255us  cuDeviceGetUuid\n",
            "                    0.00%  11.251us         1  11.251us  11.251us  11.251us  cuDeviceGetName\n",
            "                    0.00%  6.4690us         1  6.4690us  6.4690us  6.4690us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.6610us         1  4.6610us  4.6610us  4.6610us  cuDeviceTotalMem\n",
            "                    0.00%  2.1950us         3     731ns     246ns  1.6580us  cuDeviceGetCount\n",
            "                    0.00%  1.3770us         2     688ns     302ns  1.0750us  cuDeviceGet\n",
            "                    0.00%     735ns         1     735ns     735ns     735ns  cuModuleGetLoadingMode\n",
            "\n",
            "==838== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      94  87.148KB  4.0000KB  984.00KB  8.000000MB  918.6810us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  360.7660us  Device To Host\n",
            "      11         -         -         -           -  2.601003ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7Tz-xo3d1oX"
      },
      "source": [
        "That's another big speedup from running multiple blocks! (Note your results may vary from the blog post due to whatever GPU you've been allocated by Colab. If you notice your speedups for the final example are not as drastic as those in the blog post, check out #4 in the *Exercises* section below.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja5CiQZpicHC"
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEijwk25id3t"
      },
      "source": [
        "To keep you going, here are a few things to try on your own.\n",
        "\n",
        "1. Browse the [CUDA Toolkit documentation](https://docs.nvidia.com/cuda/index.html). If you haven’t installed CUDA yet, check out the [Quick Start Guide](https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html) and the installation guides. Then browse the [Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) and the [Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html). There are also tuning guides for various architectures.\n",
        "2. Experiment with `printf()` inside the kernel. Try printing out the values of `threadIdx.x` and `blockIdx.x` for some or all of the threads. Do they print in sequential order? Why or why not?\n",
        "3. Print the value of `threadIdx.y` or `threadIdx.z` (or `blockIdx.y`) in the kernel. (Likewise for `blockDim` and `gridDim`). Why do these exist? How do you get them to take on values other than 0 (1 for the dims)?\n",
        "4. If you have access to a [Pascal-based GPU](https://developer.nvidia.com/blog/inside-pascal/), try running `add_grid.cu` on it. Is performance better or worse than the K80 results? Why? (Hint: read about [Pascal’s Page Migration Engine and the CUDA 8 Unified Memory API](https://developer.nvidia.com/blog/beyond-gpu-memory-limits-unified-memory-pascal/).) For a detailed answer to this question, see the post [Unified Memory for CUDA Beginners](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpWVnIPujp0K"
      },
      "source": [
        "## Where to From Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTyQePjlkRJ3"
      },
      "source": [
        "If you enjoyed this notebook and want to learn more, the [NVIDIA DLI](https://nvidia.com/dli) offers several in depth CUDA Programming courses.\n",
        "\n",
        "For those of you just starting out, please consider [_Fundamentals of Accelerated Computing with CUDA C/C++_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/about) which provides dedicated GPU resources, a more sophisticated programming environment, use of the [NVIDIA Nsight Systems™](https://developer.nvidia.com/nsight-systems) visual profiler, dozens of interactive exercises, detailed presentations, over 8 hours of material, and the ability to earn a DLI Certificate of Competency.\n",
        "\n",
        "Similarly, for Python programmers, please consider [_Fundamentals of Accelerated Computing with CUDA Python_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-02+V1/about).\n",
        "\n",
        "For more intermediate and advance CUDA programming materials, please check out the _Accelerated Computing_ section of the NVIDIA DLI [self-paced catalog](https://www.nvidia.com/en-us/training/online/)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exercise2.cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "__global__ void myKernel()\n",
        "{\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    printf(\"Element index: %d, Thread index: %d, Block index: %d\\n\", tid, threadIdx.x, blockIdx.x);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Launch kernel with 3 blocks, each containing 4 threads\n",
        "    dim3 gridDim(3);\n",
        "    dim3 blockDim(4);\n",
        "    myKernel<<<gridDim, blockDim>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "VAoU_kvxbvzX",
        "outputId": "5a82d756-d1e0-45e9-8af9-dbbddaf803db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting exercise2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc exercise2.cu -o exercise2\n",
        "nvprof ./exercise2"
      ],
      "metadata": {
        "id": "ASehI23tb8m7",
        "outputId": "47b43668-21e0-4f68-e151-42dd20a85c00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==8894== NVPROF is profiling process 8894, command: ./exercise2\n",
            "Element index: 8, Thread index: 0, Block index: 2\n",
            "Element index: 9, Thread index: 1, Block index: 2\n",
            "Element index: 10, Thread index: 2, Block index: 2\n",
            "Element index: 11, Thread index: 3, Block index: 2\n",
            "Element index: 0, Thread index: 0, Block index: 0\n",
            "Element index: 1, Thread index: 1, Block index: 0\n",
            "Element index: 2, Thread index: 2, Block index: 0\n",
            "Element index: 3, Thread index: 3, Block index: 0\n",
            "Element index: 4, Thread index: 0, Block index: 1\n",
            "Element index: 5, Thread index: 1, Block index: 1\n",
            "Element index: 6, Thread index: 2, Block index: 1\n",
            "Element index: 7, Thread index: 3, Block index: 1\n",
            "==8894== Profiling application: ./exercise2\n",
            "==8894== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  112.58us         1  112.58us  112.58us  112.58us  myKernel(void)\n",
            "      API calls:   99.86%  231.30ms         1  231.30ms  231.30ms  231.30ms  cudaLaunchKernel\n",
            "                    0.07%  157.42us         1  157.42us  157.42us  157.42us  cudaDeviceSynchronize\n",
            "                    0.06%  131.56us       114  1.1540us     147ns  51.360us  cuDeviceGetAttribute\n",
            "                    0.00%  11.383us         1  11.383us  11.383us  11.383us  cuDeviceGetName\n",
            "                    0.00%  5.7560us         1  5.7560us  5.7560us  5.7560us  cuDeviceGetPCIBusId\n",
            "                    0.00%  5.1080us         1  5.1080us  5.1080us  5.1080us  cuDeviceTotalMem\n",
            "                    0.00%  2.6470us         2  1.3230us     205ns  2.4420us  cuDeviceGet\n",
            "                    0.00%  1.7440us         3     581ns     210ns  1.2950us  cuDeviceGetCount\n",
            "                    0.00%     598ns         1     598ns     598ns     598ns  cuModuleGetLoadingMode\n",
            "                    0.00%     220ns         1     220ns     220ns     220ns  cuDeviceGetUuid\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the kernel will be executed with a total of 12 threads (3 blocks * 4 threads per block).\n",
        "\n",
        "While the threads within each block will execute in order (from threadIdx.x = 0 to blockDim.x - 1), the order in which the blocks are executed and the threads within each block are scheduled is not deterministic.\n",
        "\n",
        "Therefore, the output values of threadIdx.x and blockIdx.x may appear in a non-sequential order."
      ],
      "metadata": {
        "id": "FrAD5RU3ehki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exercise3.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "__global__ void myKernel()\n",
        "{\n",
        "    printf(\"Thread index: (%d, %d, %d), Block index: (%d, %d, %d)\\n\",\n",
        "           threadIdx.x, threadIdx.y, threadIdx.z,\n",
        "           blockIdx.x, blockIdx.y, blockIdx.z);\n",
        "\n",
        "    printf(\"Block dimensions: (%d, %d, %d), Grid dimensions: (%d, %d, %d)\\n\",\n",
        "           blockDim.x, blockDim.y, blockDim.z,\n",
        "           gridDim.x, gridDim.y, gridDim.z);\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "    // dim3 gridDim(2);\n",
        "    // dim3 blockDim(3);\n",
        "    // Launch kernel with a 2x2x2 grid and a 3x3x3 block\n",
        "    dim3 gridDim(2, 2, 2);\n",
        "    dim3 blockDim(3, 3, 3);\n",
        "    myKernel<<<gridDim, blockDim>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "ZVbe5lsDZxHg",
        "outputId": "45000418-fad6-4c6c-b67c-06b71409b0bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting exercise3.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc exercise3.cu -o exercise3\n",
        "nvprof ./exercise3"
      ],
      "metadata": {
        "id": "xem5CkO3acpf",
        "outputId": "d678653b-6874-46b1-9776-fd29adf3c933",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==11230== NVPROF is profiling process 11230, command: ./exercise3\n",
            "Thread index: (0, 0, 0), Block index: (1, 1, 1)\n",
            "Thread index: (1, 0, 0), Block index: (1, 1, 1)\n",
            "Thread index: (2, 0, 0), Block index: (1, 1, 1)\n",
            "Thread index: (0, 1, 0), Block index: (1, 1, 1)\n",
            "Thread index: (1, 1, 0), Block index: (1, 1, 1)\n",
            "Thread index: (2, 1, 0), Block index: (1, 1, 1)\n",
            "Thread index: (0, 2, 0), Block index: (1, 1, 1)\n",
            "Thread index: (1, 2, 0), Block index: (1, 1, 1)\n",
            "Thread index: (2, 2, 0), Block index: (1, 1, 1)\n",
            "Thread index: (0, 0, 1), Block index: (1, 1, 1)\n",
            "Thread index: (1, 0, 1), Block index: (1, 1, 1)\n",
            "Thread index: (2, 0, 1), Block index: (1, 1, 1)\n",
            "Thread index: (0, 1, 1), Block index: (1, 1, 1)\n",
            "Thread index: (1, 1, 1), Block index: (1, 1, 1)\n",
            "Thread index: (2, 1, 1), Block index: (1, 1, 1)\n",
            "Thread index: (0, 2, 1), Block index: (1, 1, 1)\n",
            "Thread index: (1, 2, 1), Block index: (1, 1, 1)\n",
            "Thread index: (2, 2, 1), Block index: (1, 1, 1)\n",
            "Thread index: (0, 0, 2), Block index: (1, 1, 1)\n",
            "Thread index: (1, 0, 2), Block index: (1, 1, 1)\n",
            "Thread index: (2, 0, 2), Block index: (1, 1, 1)\n",
            "Thread index: (0, 1, 2), Block index: (1, 1, 1)\n",
            "Thread index: (1, 1, 2), Block index: (1, 1, 1)\n",
            "Thread index: (2, 1, 2), Block index: (1, 1, 1)\n",
            "Thread index: (0, 2, 2), Block index: (1, 1, 1)\n",
            "Thread index: (1, 2, 2), Block index: (1, 1, 1)\n",
            "Thread index: (2, 2, 2), Block index: (1, 1, 1)\n",
            "Thread index: (0, 0, 0), Block index: (0, 1, 0)\n",
            "Thread index: (1, 0, 0), Block index: (0, 1, 0)\n",
            "Thread index: (2, 0, 0), Block index: (0, 1, 0)\n",
            "Thread index: (0, 1, 0), Block index: (0, 1, 0)\n",
            "Thread index: (1, 1, 0), Block index: (0, 1, 0)\n",
            "Thread index: (2, 1, 0), Block index: (0, 1, 0)\n",
            "Thread index: (0, 2, 0), Block index: (0, 1, 0)\n",
            "Thread index: (1, 2, 0), Block index: (0, 1, 0)\n",
            "Thread index: (2, 2, 0), Block index: (0, 1, 0)\n",
            "Thread index: (0, 0, 1), Block index: (0, 1, 0)\n",
            "Thread index: (1, 0, 1), Block index: (0, 1, 0)\n",
            "Thread index: (2, 0, 1), Block index: (0, 1, 0)\n",
            "Thread index: (0, 1, 1), Block index: (0, 1, 0)\n",
            "Thread index: (1, 1, 1), Block index: (0, 1, 0)\n",
            "Thread index: (2, 1, 1), Block index: (0, 1, 0)\n",
            "Thread index: (0, 2, 1), Block index: (0, 1, 0)\n",
            "Thread index: (1, 2, 1), Block index: (0, 1, 0)\n",
            "Thread index: (2, 2, 1), Block index: (0, 1, 0)\n",
            "Thread index: (0, 0, 2), Block index: (0, 1, 0)\n",
            "Thread index: (1, 0, 2), Block index: (0, 1, 0)\n",
            "Thread index: (2, 0, 2), Block index: (0, 1, 0)\n",
            "Thread index: (0, 1, 2), Block index: (0, 1, 0)\n",
            "Thread index: (1, 1, 2), Block index: (0, 1, 0)\n",
            "Thread index: (2, 1, 2), Block index: (0, 1, 0)\n",
            "Thread index: (0, 2, 2), Block index: (0, 1, 0)\n",
            "Thread index: (1, 2, 2), Block index: (0, 1, 0)\n",
            "Thread index: (2, 2, 2), Block index: (0, 1, 0)\n",
            "Thread index: (0, 0, 0), Block index: (1, 0, 1)\n",
            "Thread index: (1, 0, 0), Block index: (1, 0, 1)\n",
            "Thread index: (2, 0, 0), Block index: (1, 0, 1)\n",
            "Thread index: (0, 1, 0), Block index: (1, 0, 1)\n",
            "Thread index: (1, 1, 0), Block index: (1, 0, 1)\n",
            "Thread index: (2, 1, 0), Block index: (1, 0, 1)\n",
            "Thread index: (0, 2, 0), Block index: (1, 0, 1)\n",
            "Thread index: (1, 2, 0), Block index: (1, 0, 1)\n",
            "Thread index: (2, 2, 0), Block index: (1, 0, 1)\n",
            "Thread index: (0, 0, 1), Block index: (1, 0, 1)\n",
            "Thread index: (1, 0, 1), Block index: (1, 0, 1)\n",
            "Thread index: (2, 0, 1), Block index: (1, 0, 1)\n",
            "Thread index: (0, 1, 1), Block index: (1, 0, 1)\n",
            "Thread index: (1, 1, 1), Block index: (1, 0, 1)\n",
            "Thread index: (2, 1, 1), Block index: (1, 0, 1)\n",
            "Thread index: (0, 2, 1), Block index: (1, 0, 1)\n",
            "Thread index: (1, 2, 1), Block index: (1, 0, 1)\n",
            "Thread index: (2, 2, 1), Block index: (1, 0, 1)\n",
            "Thread index: (0, 0, 2), Block index: (1, 0, 1)\n",
            "Thread index: (1, 0, 2), Block index: (1, 0, 1)\n",
            "Thread index: (2, 0, 2), Block index: (1, 0, 1)\n",
            "Thread index: (0, 1, 2), Block index: (1, 0, 1)\n",
            "Thread index: (1, 1, 2), Block index: (1, 0, 1)\n",
            "Thread index: (2, 1, 2), Block index: (1, 0, 1)\n",
            "Thread index: (0, 2, 2), Block index: (1, 0, 1)\n",
            "Thread index: (1, 2, 2), Block index: (1, 0, 1)\n",
            "Thread index: (2, 2, 2), Block index: (1, 0, 1)\n",
            "Thread index: (0, 0, 0), Block index: (0, 0, 0)\n",
            "Thread index: (1, 0, 0), Block index: (0, 0, 0)\n",
            "Thread index: (2, 0, 0), Block index: (0, 0, 0)\n",
            "Thread index: (0, 1, 0), Block index: (0, 0, 0)\n",
            "Thread index: (1, 1, 0), Block index: (0, 0, 0)\n",
            "Thread index: (2, 1, 0), Block index: (0, 0, 0)\n",
            "Thread index: (0, 2, 0), Block index: (0, 0, 0)\n",
            "Thread index: (1, 2, 0), Block index: (0, 0, 0)\n",
            "Thread index: (2, 2, 0), Block index: (0, 0, 0)\n",
            "Thread index: (0, 0, 1), Block index: (0, 0, 0)\n",
            "Thread index: (1, 0, 1), Block index: (0, 0, 0)\n",
            "Thread index: (2, 0, 1), Block index: (0, 0, 0)\n",
            "Thread index: (0, 1, 1), Block index: (0, 0, 0)\n",
            "Thread index: (1, 1, 1), Block index: (0, 0, 0)\n",
            "Thread index: (2, 1, 1), Block index: (0, 0, 0)\n",
            "Thread index: (0, 2, 1), Block index: (0, 0, 0)\n",
            "Thread index: (1, 2, 1), Block index: (0, 0, 0)\n",
            "Thread index: (2, 2, 1), Block index: (0, 0, 0)\n",
            "Thread index: (0, 0, 2), Block index: (0, 0, 0)\n",
            "Thread index: (1, 0, 2), Block index: (0, 0, 0)\n",
            "Thread index: (2, 0, 2), Block index: (0, 0, 0)\n",
            "Thread index: (0, 1, 2), Block index: (0, 0, 0)\n",
            "Thread index: (1, 1, 2), Block index: (0, 0, 0)\n",
            "Thread index: (2, 1, 2), Block index: (0, 0, 0)\n",
            "Thread index: (0, 2, 2), Block index: (0, 0, 0)\n",
            "Thread index: (1, 2, 2), Block index: (0, 0, 0)\n",
            "Thread index: (2, 2, 2), Block index: (0, 0, 0)\n",
            "Thread index: (0, 0, 0), Block index: (0, 1, 1)\n",
            "Thread index: (1, 0, 0), Block index: (0, 1, 1)\n",
            "Thread index: (2, 0, 0), Block index: (0, 1, 1)\n",
            "Thread index: (0, 1, 0), Block index: (0, 1, 1)\n",
            "Thread index: (1, 1, 0), Block index: (0, 1, 1)\n",
            "Thread index: (2, 1, 0), Block index: (0, 1, 1)\n",
            "Thread index: (0, 2, 0), Block index: (0, 1, 1)\n",
            "Thread index: (1, 2, 0), Block index: (0, 1, 1)\n",
            "Thread index: (2, 2, 0), Block index: (0, 1, 1)\n",
            "Thread index: (0, 0, 1), Block index: (0, 1, 1)\n",
            "Thread index: (1, 0, 1), Block index: (0, 1, 1)\n",
            "Thread index: (2, 0, 1), Block index: (0, 1, 1)\n",
            "Thread index: (0, 1, 1), Block index: (0, 1, 1)\n",
            "Thread index: (1, 1, 1), Block index: (0, 1, 1)\n",
            "Thread index: (2, 1, 1), Block index: (0, 1, 1)\n",
            "Thread index: (0, 2, 1), Block index: (0, 1, 1)\n",
            "Thread index: (1, 2, 1), Block index: (0, 1, 1)\n",
            "Thread index: (2, 2, 1), Block index: (0, 1, 1)\n",
            "Thread index: (0, 0, 2), Block index: (0, 1, 1)\n",
            "Thread index: (1, 0, 2), Block index: (0, 1, 1)\n",
            "Thread index: (2, 0, 2), Block index: (0, 1, 1)\n",
            "Thread index: (0, 1, 2), Block index: (0, 1, 1)\n",
            "Thread index: (1, 1, 2), Block index: (0, 1, 1)\n",
            "Thread index: (2, 1, 2), Block index: (0, 1, 1)\n",
            "Thread index: (0, 2, 2), Block index: (0, 1, 1)\n",
            "Thread index: (1, 2, 2), Block index: (0, 1, 1)\n",
            "Thread index: (2, 2, 2), Block index: (0, 1, 1)\n",
            "Thread index: (0, 0, 0), Block index: (1, 0, 0)\n",
            "Thread index: (1, 0, 0), Block index: (1, 0, 0)\n",
            "Thread index: (2, 0, 0), Block index: (1, 0, 0)\n",
            "Thread index: (0, 1, 0), Block index: (1, 0, 0)\n",
            "Thread index: (1, 1, 0), Block index: (1, 0, 0)\n",
            "Thread index: (2, 1, 0), Block index: (1, 0, 0)\n",
            "Thread index: (0, 2, 0), Block index: (1, 0, 0)\n",
            "Thread index: (1, 2, 0), Block index: (1, 0, 0)\n",
            "Thread index: (2, 2, 0), Block index: (1, 0, 0)\n",
            "Thread index: (0, 0, 1), Block index: (1, 0, 0)\n",
            "Thread index: (1, 0, 1), Block index: (1, 0, 0)\n",
            "Thread index: (2, 0, 1), Block index: (1, 0, 0)\n",
            "Thread index: (0, 1, 1), Block index: (1, 0, 0)\n",
            "Thread index: (1, 1, 1), Block index: (1, 0, 0)\n",
            "Thread index: (2, 1, 1), Block index: (1, 0, 0)\n",
            "Thread index: (0, 2, 1), Block index: (1, 0, 0)\n",
            "Thread index: (1, 2, 1), Block index: (1, 0, 0)\n",
            "Thread index: (2, 2, 1), Block index: (1, 0, 0)\n",
            "Thread index: (0, 0, 2), Block index: (1, 0, 0)\n",
            "Thread index: (1, 0, 2), Block index: (1, 0, 0)\n",
            "Thread index: (2, 0, 2), Block index: (1, 0, 0)\n",
            "Thread index: (0, 1, 2), Block index: (1, 0, 0)\n",
            "Thread index: (1, 1, 2), Block index: (1, 0, 0)\n",
            "Thread index: (2, 1, 2), Block index: (1, 0, 0)\n",
            "Thread index: (0, 2, 2), Block index: (1, 0, 0)\n",
            "Thread index: (1, 2, 2), Block index: (1, 0, 0)\n",
            "Thread index: (2, 2, 2), Block index: (1, 0, 0)\n",
            "Thread index: (0, 0, 0), Block index: (0, 0, 1)\n",
            "Thread index: (1, 0, 0), Block index: (0, 0, 1)\n",
            "Thread index: (2, 0, 0), Block index: (0, 0, 1)\n",
            "Thread index: (0, 1, 0), Block index: (0, 0, 1)\n",
            "Thread index: (1, 1, 0), Block index: (0, 0, 1)\n",
            "Thread index: (2, 1, 0), Block index: (0, 0, 1)\n",
            "Thread index: (0, 2, 0), Block index: (0, 0, 1)\n",
            "Thread index: (1, 2, 0), Block index: (0, 0, 1)\n",
            "Thread index: (2, 2, 0), Block index: (0, 0, 1)\n",
            "Thread index: (0, 0, 1), Block index: (0, 0, 1)\n",
            "Thread index: (1, 0, 1), Block index: (0, 0, 1)\n",
            "Thread index: (2, 0, 1), Block index: (0, 0, 1)\n",
            "Thread index: (0, 1, 1), Block index: (0, 0, 1)\n",
            "Thread index: (1, 1, 1), Block index: (0, 0, 1)\n",
            "Thread index: (2, 1, 1), Block index: (0, 0, 1)\n",
            "Thread index: (0, 2, 1), Block index: (0, 0, 1)\n",
            "Thread index: (1, 2, 1), Block index: (0, 0, 1)\n",
            "Thread index: (2, 2, 1), Block index: (0, 0, 1)\n",
            "Thread index: (0, 0, 2), Block index: (0, 0, 1)\n",
            "Thread index: (1, 0, 2), Block index: (0, 0, 1)\n",
            "Thread index: (2, 0, 2), Block index: (0, 0, 1)\n",
            "Thread index: (0, 1, 2), Block index: (0, 0, 1)\n",
            "Thread index: (1, 1, 2), Block index: (0, 0, 1)\n",
            "Thread index: (2, 1, 2), Block index: (0, 0, 1)\n",
            "Thread index: (0, 2, 2), Block index: (0, 0, 1)\n",
            "Thread index: (1, 2, 2), Block index: (0, 0, 1)\n",
            "Thread index: (2, 2, 2), Block index: (0, 0, 1)\n",
            "Thread index: (0, 0, 0), Block index: (1, 1, 0)\n",
            "Thread index: (1, 0, 0), Block index: (1, 1, 0)\n",
            "Thread index: (2, 0, 0), Block index: (1, 1, 0)\n",
            "Thread index: (0, 1, 0), Block index: (1, 1, 0)\n",
            "Thread index: (1, 1, 0), Block index: (1, 1, 0)\n",
            "Thread index: (2, 1, 0), Block index: (1, 1, 0)\n",
            "Thread index: (0, 2, 0), Block index: (1, 1, 0)\n",
            "Thread index: (1, 2, 0), Block index: (1, 1, 0)\n",
            "Thread index: (2, 2, 0), Block index: (1, 1, 0)\n",
            "Thread index: (0, 0, 1), Block index: (1, 1, 0)\n",
            "Thread index: (1, 0, 1), Block index: (1, 1, 0)\n",
            "Thread index: (2, 0, 1), Block index: (1, 1, 0)\n",
            "Thread index: (0, 1, 1), Block index: (1, 1, 0)\n",
            "Thread index: (1, 1, 1), Block index: (1, 1, 0)\n",
            "Thread index: (2, 1, 1), Block index: (1, 1, 0)\n",
            "Thread index: (0, 2, 1), Block index: (1, 1, 0)\n",
            "Thread index: (1, 2, 1), Block index: (1, 1, 0)\n",
            "Thread index: (2, 2, 1), Block index: (1, 1, 0)\n",
            "Thread index: (0, 0, 2), Block index: (1, 1, 0)\n",
            "Thread index: (1, 0, 2), Block index: (1, 1, 0)\n",
            "Thread index: (2, 0, 2), Block index: (1, 1, 0)\n",
            "Thread index: (0, 1, 2), Block index: (1, 1, 0)\n",
            "Thread index: (1, 1, 2), Block index: (1, 1, 0)\n",
            "Thread index: (2, 1, 2), Block index: (1, 1, 0)\n",
            "Thread index: (0, 2, 2), Block index: (1, 1, 0)\n",
            "Thread index: (1, 2, 2), Block index: (1, 1, 0)\n",
            "Thread index: (2, 2, 2), Block index: (1, 1, 0)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "Block dimensions: (3, 3, 3), Grid dimensions: (2, 2, 2)\n",
            "==11230== Profiling application: ./exercise3\n",
            "==11230== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  456.83us         1  456.83us  456.83us  456.83us  myKernel(void)\n",
            "      API calls:   75.15%  215.56ms         1  215.56ms  215.56ms  215.56ms  cudaLaunchKernel\n",
            "                   24.74%  70.953ms         1  70.953ms  70.953ms  70.953ms  cudaDeviceSynchronize\n",
            "                    0.10%  281.50us       114  2.4690us     194ns  101.53us  cuDeviceGetAttribute\n",
            "                    0.01%  27.923us         1  27.923us  27.923us  27.923us  cuDeviceGetName\n",
            "                    0.00%  7.5800us         1  7.5800us  7.5800us  7.5800us  cuDeviceGetPCIBusId\n",
            "                    0.00%  6.4500us         1  6.4500us  6.4500us  6.4500us  cuDeviceTotalMem\n",
            "                    0.00%  2.5310us         3     843ns     382ns  1.7100us  cuDeviceGetCount\n",
            "                    0.00%  1.0020us         2     501ns     212ns     790ns  cuDeviceGet\n",
            "                    0.00%     695ns         1     695ns     695ns     695ns  cuModuleGetLoadingMode\n",
            "                    0.00%     442ns         1     442ns     442ns     442ns  cuDeviceGetUuid\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}